{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Objective: Learn how to predict apartments' renting price in Champaign, IL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 1: Data Preprocessing\n",
    "We need to preprocess data before we start running machine learning algorithm. The reasons are some observations are invalid (missing rent number) or some columns do not provide enough information (like image_urls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to load raw csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current directory\n",
    "getwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data from /home/zhang303/Apartments\n",
    "# here, we read the data crawled from place of mine at 2017-06-13 (rents-2017-06-13.csv)\n",
    "# raw is a dataframe that consists of 305278 data\n",
    "raw <- read.csv(\"/home/zhang303/ACE_592/Lab_5_ML_Prediction/rents_2017-06-13.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find every apartment in Champaign, IL. To do that, we can do some basic queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab all the data that meets the criteria\n",
    "data = raw[raw$state == \"IL\" & raw$city == \"Champaign\",]\n",
    "states = \"IL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[4,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(data)\n",
    "ncol(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Champaign should be a dataframe that contains 150 observations with 22 variables. Now we need to integrate the raw data with the census block data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function that translates a state into a code.\n",
    "# for example, codeForState(IL) = 17\n",
    "codeForState <- function (state_name) {\n",
    "  if (!exists('stateCodeMap')) {\n",
    "    stateCodeMap = read.csv('/home/zhang303/ACE_592/Lab_5_ML_Prediction/state_code_map.csv')\n",
    "  }\n",
    "  code = stateCodeMap[which(stateCodeMap$state==state_name),\"code\"]\n",
    "  if (nchar(code) < 2) {\n",
    "    code = paste0(\"0\",code)\n",
    "  }\n",
    "  return(code)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "codeForState(\"IL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get POP data for Illinois\n",
    "getPOPDataForState <- function (data,state_name) {\n",
    "  # Get layer name & path\n",
    "  layer_name = paste0(\"tl_2016_\",codeForState(state_name),\"_tabblock10\")\n",
    "  shape_object_path = paste0(\"/home/zhang303/ACE_592/Lab_5_ML_Prediction/\",codeForState(state_name),\"_shp.rds\")\n",
    "  \n",
    "  if (!file.exists(shape_object_path)) {\n",
    "    return(NA)\n",
    "  }\n",
    "  \n",
    "  # Grab just rows for the state we're processing\n",
    "  state_subset = subset(data,state==state_name)\n",
    "  \n",
    "  # If there are no rows for this state, do nothing\n",
    "  if(dim(state_subset)[1]==0) {\n",
    "    return(NA)\n",
    "  } else {\n",
    "    points_raw = state_subset\n",
    "    points_raw$lat = as.numeric(points_raw$lat)\n",
    "    points_raw$lon = as.numeric(points_raw$lon)\n",
    "      \n",
    "    source(\"/home/zhang303/ACE_592/Lab_5_ML_Prediction/PointsOverPolygons.R\",local = TRUE)\n",
    "    if (!exists(\"aug_points\")) {\n",
    "      stop(\"PointsOverPolygons.R failed to create membership data.\")\n",
    "    }\n",
    "  }\n",
    "  return(aug_points)\n",
    "}\n",
    "\n",
    "# usage in the next cell\n",
    "# example output in the 3 cells later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in real project we run every state in the U.S., but here we only take illinois for now\n",
    "# don't worry about the warning messages\n",
    "aug_data_by_state = lapply(\"IL\",function(state_name) {\n",
    "  print(paste0(\"Working on \",state_name,\"...\"))\n",
    "    \n",
    "  if (state_name == \"\") {\n",
    "    return(NA)\n",
    "  }\n",
    "    \n",
    "  aug_data = getPOPDataForState(data,state_name)\n",
    "    \n",
    "  return(aug_data)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove any states that had no rows in them\n",
    "aug_data_by_state_no_NAs = aug_data_by_state[which(!is.na(aug_data_by_state))]\n",
    "aug_data = Reduce(rbind,aug_data_by_state_no_NAs,aug_data_by_state_no_NAs[[1]][c(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data[4,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After integration, we can remove columns that do not have enough information such as image_urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that removes useless variable and categorizing square ft\n",
    "cleanChunk = function (chunk_old) {\n",
    "\n",
    "  chunk=data.frame(matrix(nrow=nrow(chunk_old),ncol=0))\n",
    "    \n",
    "  #chunk$id <- chunk_old$csv_id\n",
    "  chunk$Lat_Num <- as.numeric(chunk_old$lat)\n",
    "  chunk$Lon_Num <- as.numeric(chunk_old$lon)\n",
    "  chunk$Sqft_Num <- as.numeric(chunk_old$sqft)\n",
    "  chunk$Score_Num <- as.numeric(chunk_old$score)\n",
    "  chunk_old$rent1 <- sub(\"$\",'',as.character(chunk_old$rent),fixed=TRUE)\n",
    "  chunk$Rent_Num <- as.numeric(sub(\",\",'',as.character(chunk_old$rent1),fixed=TRUE))\n",
    "  chunk$Zip_Fac <- as.factor(chunk_old$zipcode)\n",
    "  chunk$Beds_Num <- as.numeric(chunk_old$beds)\n",
    "  chunk$Baths_Num <- as.numeric(chunk_old$baths)\n",
    "  chunk$Beds_Fac <- as.factor(chunk_old$beds)\n",
    "  chunk$Baths_Fac <- as.factor(chunk_old$baths)\n",
    "  chunk$DateAdded_Str <- as.character(chunk_old$Date.Added)\n",
    "\n",
    "  chunk$State_Fac <- as.factor(chunk_old$STATEFP10)\n",
    "  chunk$Tract_Fac <- as.factor(chunk_old$TRACTCE10)\n",
    "  chunk$Block_Fac <- as.factor(chunk_old$BLOCKCE10)\n",
    "  chunk$Date_Fac <- as.factor(chunk_old$Date.Added)\n",
    "\n",
    "  chunk$Sqft0500 <- as.factor(chunk$Sqft_Num<500)\n",
    "  chunk$Sqft500600 <- as.factor(chunk$Sqft_Num>=500 & chunk$Sqft_Num<600)\n",
    "  chunk$Sqft600700 <- as.factor(chunk$Sqft_Num>=600 & chunk$Sqft_Num<700)\n",
    "  chunk$Sqft700800 <- as.factor(chunk$Sqft_Num>=700 & chunk$Sqft_Num<800)\n",
    "  chunk$Sqft800900 <- as.factor(chunk$Sqft_Num>=800 & chunk$Sqft_Num<900)\n",
    "  chunk$Sqft9001000 <- as.factor(chunk$Sqft_Num>=900 & chunk$Sqft_Num<1000)\n",
    "  chunk$Sqft10001100 <- as.factor(chunk$Sqft_Num>=1000 & chunk$Sqft_Num<1100)\n",
    "  chunk$Sqft11001200 <- as.factor(chunk$Sqft_Num>=1100 & chunk$Sqft_Num<1200)\n",
    "  chunk$Sqft12001300 <- as.factor(chunk$Sqft_Num>=1200 & chunk$Sqft_Num<1300)\n",
    "  chunk$Sqft13001400 <- as.factor(chunk$Sqft_Num>=1300 & chunk$Sqft_Num<1400)\n",
    "  chunk$Sqft14001500 <- as.factor(chunk$Sqft_Num>=1400 & chunk$Sqft_Num<1500)\n",
    "  chunk$Sqft15001600 <- as.factor(chunk$Sqft_Num>=1500 & chunk$Sqft_Num<1600)\n",
    "  chunk$Sqft16001700 <- as.factor(chunk$Sqft_Num>=1600 & chunk$Sqft_Num<1700)\n",
    "  chunk$Sqft17001800 <- as.factor(chunk$Sqft_Num>=1700 & chunk$Sqft_Num<1800)\n",
    "  chunk$Sqft18001900 <- as.factor(chunk$Sqft_Num>=1800 & chunk$Sqft_Num<1900)\n",
    "  chunk$Sqft19002000 <- as.factor(chunk$Sqft_Num>=1900 & chunk$Sqft_Num<2000)\n",
    "  chunk$Sqft2000 <- as.factor(chunk$Sqft_Num>=2000)\n",
    "  \n",
    "  chunk$Sqft20002100 <- as.factor(chunk$Sqft_Num>=2000 & chunk$Sqft_Num<2100)\n",
    "  chunk$Sqft21002200 <- as.factor(chunk$Sqft_Num>=2100 & chunk$Sqft_Num<2200)\n",
    "  chunk$Sqft22002300 <- as.factor(chunk$Sqft_Num>=2200 & chunk$Sqft_Num<2300)\n",
    "  chunk$Sqft23002400 <- as.factor(chunk$Sqft_Num>=2300 & chunk$Sqft_Num<2400)\n",
    "  chunk$Sqft24002500 <- as.factor(chunk$Sqft_Num>=2400 & chunk$Sqft_Num<2500)\n",
    "  chunk$Sqft25002600 <- as.factor(chunk$Sqft_Num>=2500 & chunk$Sqft_Num<2600)\n",
    "  chunk$Sqft26002700 <- as.factor(chunk$Sqft_Num>=2600 & chunk$Sqft_Num<2700)\n",
    "  chunk$Sqft27002800 <- as.factor(chunk$Sqft_Num>=2700 & chunk$Sqft_Num<2800)\n",
    "  chunk$Sqft28002900 <- as.factor(chunk$Sqft_Num>=2800 & chunk$Sqft_Num<2900)\n",
    "  chunk$Sqft29003000 <- as.factor(chunk$Sqft_Num>=2900 & chunk$Sqft_Num<3000)\n",
    "  chunk$Sqft3000 <- as.factor(chunk$Sqft_Num>=3000)\n",
    "  chunk$Sqft2000 <- as.factor(chunk$Sqft_Num>=2000)\n",
    "  chunk$Sqft01000 <- as.factor(chunk$Sqft_Num<1000)\n",
    "  \n",
    "  chunk$Location_GeoJSON = data.frame(matrix(NA,nrow=nrow(chunk_old),ncol=2,dimnames = list(c(),c(\"type\",\"coordinates\"))))\n",
    "  chunk$Location_GeoJSON$coordinates = as.list(data.frame(apply(chunk_old[c(\"lat\",\"lon\")],1,function(f){\n",
    "    return(c(as.numeric(f[\"lon\"]),as.numeric(f[\"lat\"])))\n",
    "  })))\n",
    "    \n",
    "  chunk$Location_GeoJSON$type = \"Point\"\n",
    "  \n",
    "  chunk <- subset(chunk, Rent_Num!=\"NA\")\n",
    "\n",
    "  print(\"finished\")\n",
    "  return(subset(chunk, Rent_Num <= 5035))\n",
    "}\n",
    "\n",
    "clean <- cleanChunk(aug_data)\n",
    "# it's fine if you see the folling warning message\n",
    "# Warning message in cleanChunk(aug_data):\n",
    "# “NAs introduced by coercion”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Modeling\n",
    "After processing the data, we split the whole dataset into train/test set so we can run machine learning algorithm on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## taking 80% as train and 20% as test\n",
    "sample_size <- floor(0.8 * nrow(clean))\n",
    "\n",
    "## set the seed to make your partition reproductible\n",
    "set.seed(123)\n",
    "\n",
    "train_ind <- sample(seq_len(nrow(clean)), size = sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train <- clean[train_ind,]\n",
    "test <- clean[-train_ind,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print first 5 rows in train\n",
    "print(train[1:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using XGBoost in this example. XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. \n",
    "Before we start training our model, first we need to translate categorical variables into numeric values. (because XGBoost doesn't take categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "library(xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get rid of Location_GeoJSON because we don't need them\n",
    "train_no_json = train[,!names(train) %in% c(\"Location_GeoJSON\")]\n",
    "test_no_json = test[,!names(test) %in% c(\"Location_GeoJSON\")]\n",
    "\n",
    "# change type of DateAdded_Str to Date\n",
    "train_no_json$DateAdded_Str <- as.Date(train_no_json$DateAdded_Str, format = \"%m-%d-%Y\")\n",
    "test_no_json$DateAdded_Str <- as.Date(test_no_json$DateAdded_Str, format = \"%m-%d-%Y\")\n",
    "\n",
    "# from columns 14 to 42, if the value is true then change it to 1, otherwise 0\n",
    "train_no_json[, 14:42] <- as.integer(train_no_json[, 14:42] == \"TRUE\")\n",
    "test_no_json[, 14:42] <- as.integer(test_no_json[, 14:42] == \"TRUE\")\n",
    "\n",
    "# store the rent we are going to predict and delete it from train data set\n",
    "output_vector = train_no_json[,\"Rent_Num\"]\n",
    "train_no_json = train_no_json[,!names(train_no_json) %in% c(\"Rent_Num\")]\n",
    "\n",
    "# store the rent we are going to predict and delete it from test data set\n",
    "true_vector = test_no_json[, \"Rent_Num\"]\n",
    "test_no_json = test_no_json[,!names(test_no_json) %in% c(\"Rent_Num\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change to matrix because XGBoost takes matrices\n",
    "train_numeric <- data.matrix(train_no_json)\n",
    "test_numeric <- data.matrix(test_no_json)\n",
    "\n",
    "dtrain <- xgb.DMatrix(data = train_numeric, label = output_vector)\n",
    "dtest <- xgb.DMatrix(data = test_numeric, label = true_vector)\n",
    "\n",
    "# we can see the rmse of train and test via using watchlist parameter \n",
    "watchlist <- list(train = dtrain, test = dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only need to put correct parameters into the model and train it. \n",
    "From the output you can see the rmse decrease after each round of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: training data\n",
    "# max.depth: maximum depth of the tree\n",
    "# eta: step size\n",
    "# gamma: minimum loss reduction\n",
    "# subsample: subsample ratio of the training instance\n",
    "# nround: number of rounds\n",
    "# watchlist: watchlist\n",
    "# objective: regression or classification\n",
    "model <- xgb.train(data = dtrain,\n",
    "                   max.depth = 300, \n",
    "                   eta = 0.1, \n",
    "                   gamma = 0.01,\n",
    "                   subsample = 0.5,\n",
    "                   nround = 50,\n",
    "                   watchlist = watchlist, \n",
    "                   print_every_n = 10,\n",
    "                   objective = \"reg:linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around with the parameters. The list of all available parameters are in here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb.importance is a function that calculates the importance of each variable\n",
    "importance_matrix <- xgb.importance(feature_names = colnames(train_numeric), model = model)\n",
    "print(importance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can start predicting the testing set and visualize the result.\n",
    "model$pred <- predict(model, test_numeric)\n",
    "test$res <- test$Rent_Num - model$pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "library(ggplot2)\n",
    "name <- \"xgboost\"\n",
    "ggplot() + ggtitle(paste(name,\".png\",sep=\"\")) +\n",
    "      labs(x=\"Rent\", y=\"Residuals\") +\n",
    "      geom_point(aes(test$Rent_Num, test$res),\n",
    "                 colour='blue', fill = \"blue\") +\n",
    "      geom_smooth(aes(test$Rent_Num, test$res), se=TRUE, fullrange=FALSE,\n",
    "                  colour='firebrick1', fill = \"firebrick1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question:\n",
    "Try to tune the model's parameters. If you increase the eta or the maximum depth, what will happend? Can you explain the reason? Are there other ways to improve the model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
